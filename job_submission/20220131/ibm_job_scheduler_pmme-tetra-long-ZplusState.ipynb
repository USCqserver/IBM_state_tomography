{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Run Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-_ixsll6g because the default path (/home/haimeng/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "from qiskit import *\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "#you have to load some account to start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haimeng/anaconda3/envs/newest_qiskit/lib/python3.8/site-packages/qiskit/providers/ibmq/ibmqfactory.py:109: UserWarning: Timestamps in IBMQ backend properties, jobs, and job results are all now in local time instead of UTC.\n",
      "  warnings.warn('Timestamps in IBMQ backend properties, jobs, and job results '\n"
     ]
    }
   ],
   "source": [
    "# IBMQ.disable_account() \n",
    "# IBMQ.disable_account()\n",
    "token0 = '316cca1ff42067b9901b6a9e90ca76496e76b4a73139954176c6fe31a45572fcd37089924cbe5c41edb96a9b4c0e92e44451ed350e2a7e45dc809d26cd752eed'\n",
    "provider = IBMQ.enable_account(token0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the files that need to be run \n",
    "The old setup works with only one folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_qasms(qasmlist):\n",
    "    def get_time(filename):\n",
    "        idgate = filename.split('/')[-1].split('_')[8].split('=')[-1]\n",
    "        return idgate\n",
    "    print(get_time(qasmlist[0]))\n",
    "    times = list(set([get_time(x) for x in qasmlist])) #remove duplicates\n",
    "    times.sort()\n",
    "    print('# of time instances=%d'%(len(times)))\n",
    "    random.shuffle(times) #randomize the timesteps\n",
    "    print(times)\n",
    "    sorted_qasm_list = []\n",
    "    for time in times:\n",
    "        sorted_qasm_list += [f for f in qasmlist if get_time(f) == time]\n",
    "    return sorted_qasm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from datetime import date\n",
    "circuitPath = r\"../../../Circuits\"\n",
    "device = \"ibmq_bogota\"\n",
    "datestr = \"20220228\"\n",
    "runtypes = [f.name for f in os.scandir(circuitPath + '/' + device + '/' + datestr) if f.is_dir() and 'QS4_Zplus' in f.name and 'Long' in f.name and 'NonMark' not in f.name and 'Spec' not in f.name]\n",
    "batchFiles = []\n",
    "\n",
    "\n",
    "TetraStates = ['tetra%d'%(i) for i in range(4)]\n",
    "TetraStates.append('randomState')\n",
    "runtype = [item for item in runtypes for i in range(5) if item.split('_')[2]==TetraStates[i] ]\n",
    "runtypes_ordered = sorted(runtype,key=lambda t:t.split('_')[2],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtypes_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "for runtype in runtypes_ordered:\n",
    "    qasmpath = circuitPath + '/' + device + '/'+ datestr + '/' + runtype + '/'\n",
    "    qasmlist = glob.glob(qasmpath + '%s*.qasm'%(runtype.split('_')[0]))\n",
    "    # sort by id gates\n",
    "    qasmlist.sort(key=lambda x: int(x.split('/')[-1].split('_')[8].split('=')[-1]))\n",
    "    print(len(qasmlist))\n",
    "    f=qasmlist[0]\n",
    "    print(f)\n",
    "    QuantumCircuit().from_qasm_file(f).draw(output='mpl')\n",
    "    sorted_qasm_list = shuffle_qasms(qasmlist)\n",
    "    # add measurement error mitigation\n",
    "    meas_qasm = glob.glob(qasmpath + '%s*.qasm'%('MeasError'))\n",
    "    sorted_qasm_list += meas_qasm\n",
    "    print(len(sorted_qasm_list))\n",
    "    batchFiles += [sorted_qasm_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(batchFiles[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = provider.backends.ibmq_bogota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pfile qiskit.transpile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qasmlist[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=qasmlist[5]\n",
    "\n",
    "print(f)\n",
    "f = QuantumCircuit().from_qasm_file(f)\n",
    "f = qiskit.transpile(f, backend = backend,  optimization_level=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.qasm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(batchFiles[i][0].split('/')[-1].split('_')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokens = {'haimeng@usc.edu':'316cca1ff42067b9901b6a9e90ca76496e76b4a73139954176c6fe31a45572fcd37089924cbe5c41edb96a9b4c0e92e44451ed350e2a7e45dc809d26cd752eed',\n",
    "          'pokharel@usc.edu' : '21acf454868c89a86938ffcb6b17cc1236f783c6e340937ab3205598608408623893094770f403dd95ba758d9864ef2135eda8f381804787e15e8ab2aedd3ba3',\n",
    "          'zhanghaimeng1994@gmail.com':'3513ac2a5100793f7e44bf520ad54b2d7bb68da03cd0f410a3bea14853644e1841de6ce86fbee2a750f814f032584d002f601fe25360b446f81339ba3a957eed',\n",
    "          'theireasychair@gmail.com':'c59327488ff0575f9fcafd3f8d8ad627a1574fcf130508ac0314f08af78afc5cc5787acd7527140cb8aeacceaf4eaf82960785df5636991287d4f086db3b7229',\n",
    "          'theireasychair@icloud.com':'e009883c3a103e728b741783f67d8586a37c0de04447151aa9493d13b3e76518f135424c482d9be2ef02759194085c8dce0db926f392ef5caa5b66f2843025a3',\n",
    "          'jinweixu@stanford.edu':'21a7ae9539b78959e11dcc1383c5691767f36b13d5d8efa01e515b76714b796903c19a61537adf98583e1cb04e4ee2584c6ba5f632b9a16cb8ad80cc6ce90ba7'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiDict = tokens\n",
    "apiInv =dict(map(reversed, apiDict.items()))\n",
    "apitokens = list(apiDict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apitokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify API and Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script checks for how many jobs are available per token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the email address corresponding to each API, this will help when these APIs expire\n",
    "apiDict = tokens\n",
    "apiInv =dict(map(reversed, apiDict.items()))\n",
    "apitokens = list(apiDict.values())\n",
    "\n",
    "\n",
    "for token in apitokens:\n",
    "    IBMQ.disable_account() #Start with no account loaded\n",
    "    provider = IBMQ.enable_account(token) #Some account must be loaded before we start\n",
    "    backend = provider.backends.ibmq_bogota\n",
    "    available_jobs = backend.job_limit().maximum_jobs - backend.job_limit().active_jobs\n",
    "    print(f\"{available_jobs} jobs available for {apiInv[token]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend.configuration() #max_experiments=75, max_shots=8192\n",
    "max_experiments=backend.configuration().max_experiments\n",
    "max_shots=backend.configuration().max_shots\n",
    "print(max_experiments,max_shots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print backend info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop=backend.properties()\n",
    "type(prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop.qubit_property(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to import backend and token information during the runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print current working directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "runRecords = '../../../Data/records/info/' #information about each job is stored in this folder\n",
    "\n",
    "def exportJobInfo(usertoken,jobid,filenames,backend):\n",
    "    global runRecords\n",
    "    now = datetime.datetime.now()\n",
    "    qindex = int(filenames[0].split('/')[-1].split('_')[1][-1])\n",
    "    with open(runRecords+jobid + '_' +usertoken+'.csv','w+') as f:\n",
    "        writer = csv.writer(f,delimiter=',')\n",
    "        writer.writerow(['token',token])\n",
    "        writer.writerow(['jobid',jobid])\n",
    "        writer.writerow(['backend',backend.properties().backend_name])\n",
    "        writer.writerow(['qubit',qindex])\n",
    "        prop = backend.properties().qubit_property(qindex)\n",
    "        for key,value in prop.items():\n",
    "            writer.writerow([key,value])\n",
    "        writer.writerow(['circuits'])\n",
    "        for item in filenames:\n",
    "            writer.writerow([item])\n",
    "        # write time stamps\n",
    "        job = backend.retrieve_job(runId)\n",
    "        writer.writerow(['job_creation',job._time_per_step['CREATING']])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_info = backend.properties().to_dict() \n",
    "backend_info['backend_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to check for available tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "#Select a available token to send the run to\n",
    "#With the current setup, loading and unloading a token takes sometime\n",
    "#sequentially check each token for availability will increase loadtime for tokens that are near the end\n",
    "#so given a list of tokens, everytime a job is sent to a token, that token will be sent to the end\n",
    "def availableToken():\n",
    "    global provider, backend, apitokens\n",
    "    token = apitokens[0]\n",
    "    available_jobs = backend.job_limit().maximum_jobs - backend.job_limit().active_jobs\n",
    "    if available_jobs > 1:\n",
    "        print(f'{available_jobs} jobs available for {apiInv[token]}')\n",
    "        return token\n",
    "\n",
    "    #if the first token has no availability then we will continue below\n",
    "    apitokens.append(apitokens.pop(0))\n",
    "    for token in apitokens:\n",
    "        IBMQ.disable_account() \n",
    "        provider = IBMQ.enable_account(token) \n",
    "        backend = provider.backends.ibmq_bogota\n",
    "        available_jobs = backend.job_limit().maximum_jobs - backend.job_limit().active_jobs\n",
    "        if available_jobs > 1:\n",
    "            print(f'{available_jobs} jobs available for {apiInv[token]}')\n",
    "            apitokens.insert(0, apitokens.pop(apitokens.index(token)))\n",
    "            return token\n",
    "\n",
    "availableToken()   \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually doing the runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with empty containers\n",
    "#dont run this block if you already have a list of runs you want to continue\n",
    "\n",
    "runDict = {} #matches each batch to the job token\n",
    "jobDict = {} #matches each batch to the entire job object\n",
    "jobsFound = set([]) #status is updated when checked\n",
    "jobsDone = set([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend.properties().backend_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start runs\n",
    "#generally we want to run all the batchFiles, but we can also choose to run only some of them:\n",
    "#todoFiles = [batchFiles[3]]\n",
    "jobsNotFound = list(set(range(0, len(batchFiles))) - jobsFound)\n",
    "\n",
    "# todoFiles = batchFiles[0::]\n",
    "# for runFiles in todoFiles:\n",
    "for n in jobsNotFound[0::]:\n",
    "    \n",
    "    runFiles = batchFiles[n]\n",
    "    token = availableToken()\n",
    "\n",
    "    allQasmList = [QuantumCircuit.from_qasm_file(file) for file in runFiles] #this needs to edited if the batch files also contain directory\n",
    "    allCircuits= assemble(allQasmList, backend, shots=8192)\n",
    "\n",
    "    job_current = backend.run(allCircuits)\n",
    "    print(job_current.status())\n",
    "\n",
    "    runId = job_current.job_id()\n",
    "#     runTokens.append(token)\n",
    "#     runIds.append(runId)\n",
    "#         runDict[batchFiles.index(runFiles)] = [token, runId] #note that the index still refers to the original batchFiles\n",
    "    runDict[n] = [token, runId]\n",
    "    #as retrieve job is not working temporarily I will save job_current in a dictionary as well\n",
    "#     jobDict[batchFiles.index(runFiles)] = job_current\n",
    "    jobDict[n] = job_current\n",
    "    print(f\"Batch {batchFiles.index(runFiles)} has been sent\")\n",
    "\n",
    "    exportJobInfo(token, runId, batchFiles[0], backend)\n",
    "\n",
    "\n",
    "print(runDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sleep\n",
    "jobsNotDone = list(set(runDict.keys()) - jobsDone)\n",
    "\n",
    "def loadToken(token):\n",
    "    global provider, backend\n",
    "    if(not IBMQ.active_account()):\n",
    "        IBMQ.load_account()\n",
    "    IBMQ.disable_account() \n",
    "    provider = IBMQ.enable_account(token) \n",
    "    backend = provider.backends.ibmq_bogota\n",
    "    return None\n",
    "\n",
    "print(\"|Batch| Q#  | Status\")\n",
    "for job in jobsNotDone:\n",
    "#     try:\n",
    "    [token, job_id] = runDict[job]\n",
    "    loadToken(token)\n",
    "\n",
    "    #get job from dictionary instead of retrieve job as that function is broken for now\n",
    "#         job_data = backend.retrieve_job(job_id)\n",
    "    #alternatively get job from the jobD|ict\n",
    "    if job in jobDict:\n",
    "        job_data = jobDict[job]\n",
    "    else:\n",
    "        job_data = backend.retrieve_job(job_id)\n",
    "    #print(f\"Batch {job} is {job_data.status()}\")\n",
    "\n",
    "    jobsFound.add(job)\n",
    "    qno = '---'\n",
    "    if str(job_data.status()) == 'JobStatus.QUEUED':\n",
    "        #print(f'queue number is {job_data.queue_info().position}')\n",
    "        qno = str(job_data.queue_info().position).zfill(3)\n",
    "\n",
    "    if str(job_data.status()) == 'JobStatus.DONE':\n",
    "        jobsDone.add(job)\n",
    "\n",
    "    st = str(job_data.status()).split('.')[1]\n",
    "    jno = str(job).zfill(3)\n",
    "    print(f'| {jno} | {qno} | {st}')\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "print(f'{len(jobsFound)} jobs found: {jobsFound}')\n",
    "print(f'{len(jobsDone)} jobs done: {jobsDone}')\n",
    "# if some batches are showing network errors, run only those batches again\n",
    "# try, excess commands above force the loop to continue even when some batches show errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Data (Run after jobs have completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = batchFiles[0][-1]\n",
    "print(filename)\n",
    "print(os.path.dirname(filename))\n",
    "run=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultRecords = '../../Data/records/results/'\n",
    "if not os.path.exists(resultRecords):\n",
    "        os.makedirs(resultRecords)\n",
    "\n",
    "#job results are returned a list of dictionaries and they will have the same order as the order of the files in that batch\n",
    "# filenames given here should also include the folder names as well\n",
    "def exportBatchJob(filenames, token, job_id, index):\n",
    "    loadToken(token)\n",
    "    #job_data = backend.retrieve_job(job_id)\n",
    "    if job in jobDict:\n",
    "        job_data = jobDict[job]\n",
    "        print('found token')\n",
    "    else:\n",
    "        job_data = backend.retrieve_job(job_id)\n",
    "        \n",
    "    # write finish time\n",
    "    with open(runRecords+job_id + '_' +token+'.csv','a') as f:\n",
    "        writer = csv.writer(f,delimiter=',')\n",
    "        writer.writerow(['job_finish_time',job_data._time_per_step['COMPLETED']])\n",
    "        \n",
    "    with open(resultRecords + f'{job_id}_{token}_results.txt', 'w') as f:\n",
    "        f.write(str(job_data.result().to_dict()))\n",
    "    \n",
    "    for i in range(0, len(filenames)):\n",
    "        result = job_data.result().get_counts()[i]\n",
    "        exportResult(token, job_id, i, filenames[i], result,index)\n",
    "        \n",
    "        \n",
    "#results will be in the form of dictionary\n",
    "#filename here should also include the foldernames\n",
    "def exportResult(token, job_id, circNo, filename, result,index):\n",
    "#     print(filename)\n",
    "    directory = os.path.dirname(filename).replace('/Circuits/', '/Data/raw/')+'_%d'%(index)+ f'/run{run}/'\n",
    "    filename_new = os.path.basename(filename).split('.qasm')[0].split('_')\n",
    "    \n",
    "    if 'MeasMainqFree' in filename_new:\n",
    "        filename_new.insert(5,str(index))\n",
    "    file = directory + '_'.join(filename_new) + f'_{job_id}.txt'\n",
    "    if not os.path.exists(os.path.dirname(file)):\n",
    "        os.makedirs(os.path.dirname(file))\n",
    "        \n",
    "    \n",
    "    with open(file, 'w') as f:\n",
    "            f.write(\"usertoken,jobid,circuit_number\\n\")\n",
    "            f.write(f\"{token},{job_id},{circNo}\\n\")\n",
    "            [f.write( '\"'+str(key)+'\"' + ',' + str(value) + '\\n') for key, value in result.items()]\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchFilesFull = [ [ inputDir + f for f in batch] for batch in batchFiles] #this wont be necessary if the filename have dir\n",
    "for job in list(runDict.keys()):\n",
    "    [token, job_id] = runDict[job]\n",
    "    #print(runDict[job])\n",
    "    file = batchFiles[job][0]\n",
    "    directory = os.path.dirname(file.replace('/Circuits/', '/Data/raw/'))\n",
    "    if os.path.exists(directory):\n",
    "        exists = os.listdir(directory)\n",
    "        run = 1 + len(exists)\n",
    "        print(run)\n",
    "    else:\n",
    "        run =2\n",
    "    exportBatchJob(batchFiles[job], token, job_id,index=job)\n",
    "    print(f\"job {job} has been exported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newest_qiskit",
   "language": "python",
   "name": "newest_qiskit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
